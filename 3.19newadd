1 ：根据ICLR2016的paper，复现了graph level的输出，暂未train，先以avgpooling的形式进行train
2 : 准备阅读2018IBM的Graph2Seq based attention,利用其idea，加attention,和graph-level的representation，作为lstm的h和c